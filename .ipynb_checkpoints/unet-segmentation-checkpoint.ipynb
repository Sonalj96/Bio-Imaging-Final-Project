{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET SEGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arxiv Link: <a href=\"https://arxiv.org/abs/1505.04597\">U-Net: Convolutional Networks for Biomedical Image Segmentation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>UNet is a fully convolutional network(FCN) that does image segmentation. Its goal is to predict each pixel's class.</li>\n",
    " \n",
    "<li>UNet is built upon the FCN and modified in a way that it yields better segmentation in medical imaging.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Architecture\n",
    "\n",
    "<img src=\"images/u-net-architecture.png\"/>\n",
    "\n",
    "<h3>UNet Architecture has 3 parts:</h3>\n",
    "<ol>\n",
    "    <li>The Contracting/Downsampling Path</li>\n",
    "    <li>Bottleneck</li>\n",
    "    <li>The Expanding/Upsampling Path</li>\n",
    "</ol>\n",
    "\n",
    "<h3>Downsampling Path: </h3> \n",
    "<ol>\n",
    "    <li>It consists of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling.</li> \n",
    "    <li>At each downsampling step we double the number of feature channels.</li>\n",
    "</ol>\n",
    "\n",
    "<h3>Upsampling Path: </h3> \n",
    "<ol>\n",
    "     <li> Every  step  in  the  expansive  path  consists  of  an  upsampling  of  the feature map followed by a 2x2 convolution (“up-convolution”), a concatenation with the correspondingly feature  map  from  the  downsampling  path,  and  two  3x3  convolutions,  each  followed by a ReLU.</li>\n",
    "</ol>\n",
    "\n",
    "<h3> Skip Connection: </h3>\n",
    "The skip connection from the downsampling path are concatenated with feature map during upsampling path. These skip connection provide local information to global information while upsampling.\n",
    "\n",
    "<h3> Final Layer: </h3>\n",
    "At the final layer a 1x1 convolution is used to map each feature vector to the desired number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Advantages\n",
    "<h3> Advantages: </h3>\n",
    "<ol>\n",
    "    <li>The UNet combines the location information from the downsampling path to finally obtain a general information combining localisation and context, which is necessary to predict a good segmentation map.</li>\n",
    "    <li>No Dense layer is used, so image sizes can be used.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Dataset\n",
    "Link: <a href=\"https://www.kaggle.com/c/data-science-bowl-2018\">Data Science Bowl 2018</a>\n",
    "Find the nuclei in divergent images to advance medical discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-99aa47d8e87e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m## Seeding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "\n",
    "## Seeding \n",
    "seed = 2019\n",
    "random.seed = seed\n",
    "np.random.seed = seed\n",
    "tf.seed = seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGen(keras.utils.Sequence):\n",
    "    def __init__(self, ids, path, batch_size=8, image_size=128):\n",
    "        self.ids = ids\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __load__(self, id_name):\n",
    "        ## Path\n",
    "        onlyImagefiles = [f for f in listdir(image_dir) if isfile(join(image_dir, f))]\n",
    "        onlyLabelfiles = [f for f in listdir(label_dir) if isfile(join(label_dir, f))]\n",
    "        onlyImagefiles.sort()\n",
    "        onlyLabelfiles.sort()\n",
    "        \n",
    "        ## Reading Image\n",
    "        image = cv2.imread(image_path, 1)\n",
    "        image = cv2.resize(image, (self.image_size, self.image_size))\n",
    "        \n",
    "        mask = np.zeros((self.image_size, self.image_size, 1))\n",
    "        \n",
    "        ## Reading Masks\n",
    "        for name in all_masks:\n",
    "            _mask_path = mask_path + name\n",
    "            _mask_image = cv2.imread(_mask_path, -1)\n",
    "            _mask_image = cv2.resize(_mask_image, (self.image_size, self.image_size)) #128x128\n",
    "            _mask_image = np.expand_dims(_mask_image, axis=-1)\n",
    "            mask = np.maximum(mask, _mask_image)\n",
    "            \n",
    "        ## Normalizaing \n",
    "        image = image/255.0\n",
    "        mask = mask/255.0\n",
    "        \n",
    "        return image, mask\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if(index+1)*self.batch_size > len(self.ids):\n",
    "            self.batch_size = len(self.ids) - index*self.batch_size\n",
    "        \n",
    "        files_batch = self.ids[index*self.batch_size : (index+1)*self.batch_size]\n",
    "        \n",
    "        image = []\n",
    "        mask  = []\n",
    "        \n",
    "        for id_name in files_batch:\n",
    "            _img, _mask = self.__load__(id_name)\n",
    "            image.append(_img)\n",
    "            mask.append(_mask)\n",
    "            \n",
    "        image = np.array(image)\n",
    "        mask  = np.array(mask)\n",
    "        \n",
    "        return image, mask\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.ids)/float(self.batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "train_path = \"dataset/stage1_train/\"\n",
    "epochs = 5\n",
    "batch_size = 8\n",
    "\n",
    "## Training Ids\n",
    "train_ids = next(os.walk(train_path))[1]\n",
    "\n",
    "## Validation Data Size\n",
    "val_data_size = 10\n",
    "\n",
    "valid_ids = train_ids[:val_data_size]\n",
    "train_ids = train_ids[val_data_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile,join\n",
    "class GetData():\n",
    "\tdef __init__(self, data_dir):\t\n",
    "\t\timages_list =[]\t\t\n",
    "\t\tlabels_list = []\t\t\n",
    "\t\tself.source_list = []\n",
    "\t\tlabel_dir = os.path.join(data_dir, \"Labels\")\n",
    "\t\timage_dir = os.path.join(data_dir, \"Images\")\n",
    "\t\tself.image_size = 128\n",
    "\t\texamples = 0\n",
    "\t\tprint(\"loading images\")\n",
    "\t\tonlyImagefiles = [f for f in listdir(image_dir) if isfile(join(image_dir, f))]\n",
    "\t\tonlyLabelfiles = [f for f in listdir(label_dir) if isfile(join(label_dir, f))]\n",
    "\t\tonlyImagefiles.sort()\n",
    "\t\tonlyLabelfiles.sort()\n",
    "\n",
    "\t\tfor i in range (len(onlyImagefiles)):\n",
    "\t\t\timage = cv2.imread(os.path.join(image_dir,onlyImagefiles[i]),cv2.IMREAD_GRAYSCALE)\n",
    "\t\t\t#im = Image.open(os.path.join(label_dir,onlyLabelfiles[i]),cv2.IMREAD_GRAYSCALE)\n",
    "\t\t\t#label = np.array(im)\n",
    "\t\t\tlabel = cv2.imread(os.path.join(label_dir,onlyLabelfiles[i]),cv2.IMREAD_GRAYSCALE)\n",
    "\t\t\timage= cv2.resize(image, (self.image_size, self.image_size))\n",
    "\t\t\tlabel= cv2.resize(label, (self.image_size, self.image_size))\n",
    "\t\t\t#image = image[96:224,96:224]\n",
    "\t\t\t#label = label[96:224,96:224]\n",
    "\t\t\t#cv2.imwrite(\"Pre_\"+str(i)+\".jpg\",label)\n",
    "\t\t\t#image = image[...,0][...,None]/255\n",
    "\t\t\tlabel = label>20\n",
    "\t\t\timage = image/255\n",
    "\t\t\timage = image[...,None]\n",
    "\t\t\tlabel = label[...,None]\n",
    "\t\t\tlabel = label.astype(np.int32)\n",
    "\t\t\t#label = label*255\n",
    "\t\t\t#cv2.imwrite(\"Post_\"+str(i)+\".jpg\",label)\n",
    "\t\t\timages_list.append(image)\n",
    "\t\t\tlabels_list.append(label)\n",
    "\t\t\texamples = examples +1\n",
    "\t\t\t\t\t\t\t\n",
    "\t\tprint(\"finished loading images\")\n",
    "\t\tself.examples = examples\n",
    "\t\tprint(\"Number of examples found: \", examples)\n",
    "\t\tself.images = np.array(images_list)\n",
    "\t\tself.labels = np.array(labels_list)\n",
    "\tdef next_batch(self, batch_size):\n",
    "\t\n",
    "\t\tif len(self.source_list) < batch_size:\n",
    "\t\t\tnew_source = list(range(self.examples))\n",
    "\t\t\trandom.shuffle(new_source)\n",
    "\t\t\tself.source_list.extend(new_source)\n",
    "\n",
    "\t\texamples_idx = self.source_list[:batch_size]\n",
    "\t\tdel self.source_list[:batch_size]\n",
    "\n",
    "\t\treturn self.images[examples_idx,...], self.labels[examples_idx,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Directory Directory \n",
    "base_dir= 'Data'\n",
    "\n",
    "# Training and Test Directories \n",
    "train_dir = os.path.join(base_dir,'Train')\n",
    "test_dir = os.path.join(base_dir,'Test')\n",
    "real_dir = os.path.join(base_dir,'Real')\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "BUFFER_SIZE = 1000\n",
    "image_size = 128\n",
    "EPOCHS = 20\n",
    "def PreProcessImages():\n",
    "\ttrain_data = GetData(train_dir)\n",
    "\ttest_data = GetData(test_dir)\n",
    "\treal_data = GetData(real_dir)\n",
    "\n",
    "\treturn train_data,  test_data, real_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,  test_data, real_data= PreProcessImages()\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.imshow(np.reshape(train_data.images[3,:,:,:], (image_size, image_size)), cmap=\"gray\")\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.imshow(np.reshape(train_data.labels[3,:,:,:], (image_size, image_size)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImportImages(train_data,  test_data):\n",
    "\ttrain_dataset=tf.data.Dataset.from_tensor_slices((train_data.images, train_data.labels)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\ttest_dataset = tf.data.Dataset.from_tensor_slices((test_data.images, test_data.labels)).batch(BATCH_SIZE)\n",
    "\t\n",
    "\treturn train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Convolutional Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
    "    p = keras.layers.MaxPool2D((2, 2), (2, 2))(c)\n",
    "    return c, p\n",
    "\n",
    "def up_block(x, skip, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    us = keras.layers.UpSampling2D((2, 2))(x)\n",
    "    concat = keras.layers.Concatenate()([us, skip])\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(concat)\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
    "    return c\n",
    "\n",
    "def bottleneck(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n",
    "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNet():\n",
    "    f = [16, 32, 64, 128, 256]\n",
    "    inputs = keras.layers.Input((image_size, image_size, 3))\n",
    "    \n",
    "    p0 = inputs\n",
    "    c1, p1 = down_block(p0, f[0]) #128 -> 64\n",
    "    c2, p2 = down_block(p1, f[1]) #64 -> 32\n",
    "    c3, p3 = down_block(p2, f[2]) #32 -> 16\n",
    "    c4, p4 = down_block(p3, f[3]) #16->8\n",
    "    \n",
    "    bn = bottleneck(p4, f[4])\n",
    "    \n",
    "    u1 = up_block(bn, c4, f[3]) #8 -> 16\n",
    "    u2 = up_block(u1, c3, f[2]) #16 -> 32\n",
    "    u3 = up_block(u2, c2, f[1]) #32 -> 64\n",
    "    u4 = up_block(u3, c1, f[0]) #64 -> 128\n",
    "    \n",
    "    outputs = keras.layers.Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(u4)\n",
    "    model = keras.models.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = UNet()\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\",f1_metric])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data,  test_data, real_data= PreProcessImages()\n",
    "train_dataset, test_dataset= ImportImages(train_data,  test_data)\n",
    "\n",
    "train_steps = len(train_data.labels)//batch_size\n",
    "valid_steps = len(test_data.labels)//batch_size\n",
    "\n",
    "#model.fit_generator(train_data, validation_data=test_data, steps_per_epoch=train_steps, validation_steps=valid_steps, \n",
    " #                   epochs=epochs)\n",
    "    \n",
    "model.fit(train_data.images,train_data.labels, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the Weights\n",
    "model.save_weights(\"UNetW.h5\")\n",
    "\n",
    "## Dataset for prediction\n",
    "x, y = valid_gen.__getitem__(1)\n",
    "result = model.predict(x)\n",
    "\n",
    "result = result > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.imshow(np.reshape(y[0]*255, (image_size, image_size)), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.imshow(np.reshape(result[0]*255, (image_size, image_size)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.imshow(np.reshape(y[1]*255, (image_size, image_size)), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.imshow(np.reshape(result[1]*255, (image_size, image_size)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
